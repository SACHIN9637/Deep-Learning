{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Detailed Comments Explanation:\n",
        "\n",
        "Sigmoid and Derivative:\n",
        "\n",
        "sigmoid(x) computes the output of the sigmoid activation function. It maps any real-valued input into a value between 0 and 1. sigmoid_derivative(x) is the derivative of the sigmoid function, which is used during backpropagation to calculate gradients.\n",
        "\n",
        "Neural Network Class:\n",
        "\n",
        "The NeuralNetwork class contains the methods to initialize the network, perform the forward and backward passes, and train the network.\n",
        "\n",
        "Forward Pass:\n",
        "\n",
        "The forward pass computes the activations of the neurons in the network. The input is passed through the layers, with the weights and biases applied, followed by the activation function (sigmoid) to compute the output.\n",
        "\n",
        "Backward Pass (Backpropagation):\n",
        "\n",
        "During the backward pass, the weights are updated by calculating the error between the predicted output and the true output. The gradients are calculated using the derivative of the sigmoid function, and the weights and biases are updated using gradient descent with a specified learning rate.\n",
        "\n",
        "Training Method:\n",
        "\n",
        "The network is trained by performing multiple epochs, where each epoch involves a forward pass followed by a backward pass. Every 1000 epochs, the loss (mean squared error) is printed to track the network's progress in learning.\n",
        "\n",
        "Main Program:\n",
        "\n",
        "The main program defines a simple XOR dataset, where the inputs are 0 and 1 combinations, and the output is their XOR result. The network is created with 2 input neurons, 4 hidden neurons, and 1 output neuron. The network is trained on the XOR data for 10,000 epochs with a learning rate of 0.1.\n",
        "\n",
        "Output:\n",
        "\n",
        "After training, the network is tested on the same XOR inputs, and the predictions are printed."
      ],
      "metadata": {
        "id": "Z7yFqgnws2WW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKmllQThrvJb",
        "outputId": "bd55fc82-2b72-4d8c-939f-bb539cdca918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of hidden layers: 3\n",
            "Enter number of neurons in hidden layer 1: 2\n",
            "Enter number of neurons in hidden layer 2: 5\n",
            "Enter number of neurons in hidden layer 3: 4\n",
            "Epoch 0 - Loss: 0.256275 - Accuracy: 50.00%\n",
            "Epoch 1000 - Loss: 0.249893 - Accuracy: 50.00%\n",
            "Epoch 2000 - Loss: 0.249842 - Accuracy: 50.00%\n",
            "Epoch 3000 - Loss: 0.249762 - Accuracy: 75.00%\n",
            "Epoch 4000 - Loss: 0.249624 - Accuracy: 75.00%\n",
            "Epoch 5000 - Loss: 0.249362 - Accuracy: 75.00%\n",
            "Epoch 6000 - Loss: 0.248779 - Accuracy: 75.00%\n",
            "Epoch 7000 - Loss: 0.247155 - Accuracy: 75.00%\n",
            "Epoch 8000 - Loss: 0.240347 - Accuracy: 75.00%\n",
            "Epoch 9000 - Loss: 0.199924 - Accuracy: 75.00%\n",
            "\n",
            "Final Predictions:\n",
            "Input: [0 0], Predicted: 0.1528, Target: 0\n",
            "Input: [0 1], Predicted: 0.6900, Target: 1\n",
            "Input: [1 0], Predicted: 0.6779, Target: 1\n",
            "Input: [1 1], Predicted: 0.4535, Target: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Numerically stable sigmoid activation function.\n",
        "    \"\"\"\n",
        "    # Clip values to avoid overflow\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of the sigmoid function.\n",
        "    Now correctly accepts the sigmoid output as input.\n",
        "    \"\"\"\n",
        "    # x should already be sigmoid(x)\n",
        "    return x * (1 - x)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers_sizes, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers_sizes = hidden_layers_sizes\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Use Xavier/Glorot initialization for better convergence\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        prev_size = self.input_size\n",
        "        for size in self.hidden_layers_sizes:\n",
        "            # Xavier initialization\n",
        "            limit = np.sqrt(6 / (prev_size + size))\n",
        "            self.weights.append(np.random.uniform(-limit, limit, (prev_size, size)))\n",
        "            self.biases.append(np.zeros((1, size)))  # Initialize biases to zero\n",
        "            prev_size = size\n",
        "\n",
        "        # Output layer initialization\n",
        "        limit = np.sqrt(6 / (prev_size + self.output_size))\n",
        "        self.weights.append(np.random.uniform(-limit, limit, (prev_size, self.output_size)))\n",
        "        self.biases.append(np.zeros((1, self.output_size)))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass with input validation.\n",
        "        \"\"\"\n",
        "        # Input validation\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = np.array(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(1, -1)\n",
        "\n",
        "        self.activations = [X]\n",
        "\n",
        "        # Forward pass through all layers\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[i], self.weights[i]) + self.biases[i]\n",
        "            a = sigmoid(z)\n",
        "            self.activations.append(a)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Backward pass with gradient clipping.\n",
        "        \"\"\"\n",
        "        # Convert y to numpy array if needed\n",
        "        if not isinstance(y, np.ndarray):\n",
        "            y = np.array(y)\n",
        "        if y.ndim == 1:\n",
        "            y = y.reshape(-1, 1)\n",
        "\n",
        "        # Initialize lists to store gradients\n",
        "        weight_gradients = []\n",
        "        bias_gradients = []\n",
        "\n",
        "        # Output layer error\n",
        "        error = y - self.activations[-1]\n",
        "        delta = error * sigmoid_derivative(self.activations[-1])\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            # Calculate gradients\n",
        "            weight_grad = np.dot(self.activations[i].T, delta)\n",
        "            bias_grad = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "            # Gradient clipping\n",
        "            weight_grad = np.clip(weight_grad, -1, 1)\n",
        "            bias_grad = np.clip(bias_grad, -1, 1)\n",
        "\n",
        "            # Store gradients\n",
        "            weight_gradients.insert(0, weight_grad)\n",
        "            bias_gradients.insert(0, bias_grad)\n",
        "\n",
        "            # Calculate delta for next layer\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(self.activations[i])\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] += learning_rate * weight_gradients[i]\n",
        "            self.biases[i] += learning_rate * bias_gradients[i]\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate, batch_size=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Training with mini-batch support and better monitoring.\n",
        "        \"\"\"\n",
        "        # Convert inputs to numpy arrays\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = len(X)\n",
        "\n",
        "        n_samples = len(X)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            # Mini-batch training\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch_X = X_shuffled[i:i + batch_size]\n",
        "                batch_y = y_shuffled[i:i + batch_size]\n",
        "\n",
        "                self.forward(batch_X)\n",
        "                self.backward(batch_X, batch_y, learning_rate)\n",
        "\n",
        "            # Print progress\n",
        "            if verbose and epoch % 1000 == 0:\n",
        "                predictions = self.forward(X)\n",
        "                mse = np.mean(np.square(y - predictions))\n",
        "                accuracy = np.mean((predictions > 0.5) == y)\n",
        "                print(f\"Epoch {epoch} - Loss: {mse:.6f} - Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # XOR problem setup\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    try:\n",
        "        # Input validation\n",
        "        num_hidden_layers = int(input(\"Enter number of hidden layers: \"))\n",
        "        if num_hidden_layers < 1:\n",
        "            raise ValueError(\"Number of hidden layers must be at least 1\")\n",
        "\n",
        "        hidden_layers_sizes = []\n",
        "        for i in range(num_hidden_layers):\n",
        "            neurons = int(input(f\"Enter number of neurons in hidden layer {i+1}: \"))\n",
        "            if neurons < 1:\n",
        "                raise ValueError(f\"Layer {i+1} must have at least 1 neuron\")\n",
        "            hidden_layers_sizes.append(neurons)\n",
        "\n",
        "        # Create and train network\n",
        "        nn = NeuralNetwork(input_size=2, hidden_layers_sizes=hidden_layers_sizes, output_size=1)\n",
        "        nn.train(X, y, epochs=10000, learning_rate=0.1, batch_size=4)\n",
        "\n",
        "        # Test network\n",
        "        predictions = nn.forward(X)\n",
        "        print(\"\\nFinal Predictions:\")\n",
        "        for input_data, pred, target in zip(X, predictions, y):\n",
        "            print(f\"Input: {input_data}, Predicted: {pred[0]:.4f}, Target: {target[0]}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ]
    }
  ]
}