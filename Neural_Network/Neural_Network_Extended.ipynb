{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Neural_Network_Extended\n",
        "\n",
        "1)Architecture:\n",
        "\n",
        "Supports customizable layers (input, hidden, output) and activation functions (e.g., ReLU, Sigmoid, Tanh).\n",
        "Xavier initialization for weights ensures efficient training.\n",
        "\n",
        "2)Training:\n",
        "\n",
        "Implements forward and backward propagation with gradient clipping to stabilize updates.\n",
        "Dropout for regularization, learning rate decay for gradual optimization.\n",
        "Batch training with optional batch size configuration.\n",
        "\n",
        "3)Evaluation:\n",
        "\n",
        "Calculates loss (mean squared error) and accuracy.\n",
        "Makes predictions with threshold-based binary classification.\n",
        "\n",
        "4)Utilities:\n",
        "\n",
        "Save and load model weights/biases using JSON.\n",
        "Plots training loss for visualization.\n",
        "\n",
        "5)Example:\n",
        "\n",
        "Solves the XOR problem with a 2-input, 1-hidden layer (4 neurons), and 1-output architecture.\n",
        "\n",
        "\n",
        "Key Outputs:\n",
        "\n",
        "Training Loss over epochs (visualized via plot_training_history).\n",
        "Final Results: Loss, accuracy, and detailed predictions for the XOR dataset.\n",
        "Strengths:\n",
        "Flexible and modular design.\n",
        "Includes dropout, learning rate decay, and gradient clipping for robustness.\n",
        "Example Use Case:\n",
        "The code is an educational implementation for solving small-scale problems like XOR classification and can be expanded for other feedforward neural network tasks."
      ],
      "metadata": {
        "id": "qYpBZDMMsKNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqj5WPTvojaB",
        "outputId": "5e6b4f66-e4b1-4a92-c5de-d7edf9974564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results:\n",
            "Loss: 0.250019\n",
            "Accuracy: 50.00%\n",
            "\n",
            "Predictions:\n",
            "Input: [0 0] -> Predicted: 0.5003 (Expected: 0)\n",
            "Input: [0 1] -> Predicted: 0.4914 (Expected: 1)\n",
            "Input: [1 0] -> Predicted: 0.5003 (Expected: 1)\n",
            "Input: [1 1] -> Predicted: 0.5003 (Expected: 0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Optional\n",
        "import json\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    A flexible implementation of a feedforward neural network using NumPy.\n",
        "    Supports multiple hidden layers, activation functions, learning rate decay, dropout, and more.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_layers_sizes: List[int], output_size: int,\n",
        "                 activations: List[str], dropout_rates: Optional[List[float]] = None):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with the specified architecture.\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_layers_sizes: List containing the size of each hidden layer\n",
        "            output_size: Number of output neurons\n",
        "            activations: List of activation functions for each layer\n",
        "            dropout_rates: List of dropout rates for each layer (optional)\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers_sizes = hidden_layers_sizes\n",
        "        self.output_size = output_size\n",
        "        self.activations = activations\n",
        "        self.dropout_rates = dropout_rates if dropout_rates else [0.0] * len(activations)\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))  # Xavier Initialization\n",
        "            self.weights.append(np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i + 1])))\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
        "\n",
        "        self.activations_cache = []\n",
        "        self.dropouts_cache = []\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_function(name: str, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply the specified activation function.\"\"\"\n",
        "        if name == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "        elif name == \"relu\":\n",
        "            return np.maximum(0, x)\n",
        "        elif name == \"tanh\":\n",
        "            return np.tanh(x)\n",
        "        raise ValueError(f\"Unsupported activation function: {name}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_derivative(name: str, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute the derivative of the specified activation function.\"\"\"\n",
        "        if name == \"sigmoid\":\n",
        "            s = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "            return s * (1 - s)\n",
        "        elif name == \"relu\":\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        elif name == \"tanh\":\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "        raise ValueError(f\"Unsupported activation function: {name}\")\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Perform forward propagation through the network.\"\"\"\n",
        "        self.activations_cache = [X]\n",
        "        self.dropouts_cache = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations_cache[-1], self.weights[i]) + self.biases[i]\n",
        "            activation = self.activation_function(self.activations[i], z)\n",
        "\n",
        "            # Apply dropout if specified\n",
        "            if self.dropout_rates[i] > 0:\n",
        "                dropout_mask = np.random.rand(*activation.shape) > self.dropout_rates[i]\n",
        "                activation = activation * dropout_mask / (1 - self.dropout_rates[i])\n",
        "                self.dropouts_cache.append(dropout_mask)\n",
        "            else:\n",
        "                self.dropouts_cache.append(None)\n",
        "\n",
        "            self.activations_cache.append(activation)\n",
        "\n",
        "        return self.activations_cache[-1]\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float) -> None:\n",
        "        \"\"\"Perform backpropagation to update weights and biases.\"\"\"\n",
        "        m = len(y)\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        error = self.activations_cache[-1] - y\n",
        "        delta = error * self.activation_derivative(self.activations[-1], self.activations_cache[-1])\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            weight_grad = np.dot(self.activations_cache[i].T, delta) / m\n",
        "            bias_grad = np.sum(delta, axis=0, keepdims=True) / m\n",
        "\n",
        "            # Apply gradient clipping\n",
        "            weight_grad = np.clip(weight_grad, -1, 1)\n",
        "            bias_grad = np.clip(bias_grad, -1, 1)\n",
        "\n",
        "            self.weights[i] -= learning_rate * weight_grad\n",
        "            self.biases[i] -= learning_rate * bias_grad\n",
        "\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(\n",
        "                    self.activations[i - 1], self.activations_cache[i]\n",
        "                )\n",
        "                # Apply dropout mask\n",
        "                if self.dropouts_cache[i - 1] is not None:\n",
        "                    delta *= self.dropouts_cache[i - 1]\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float,\n",
        "              decay_rate: float = 0.0, batch_size: Optional[int] = None, verbose: bool = True):\n",
        "        \"\"\"Train the network.\"\"\"\n",
        "        batch_size = batch_size or len(X)\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(len(X))\n",
        "            X_shuffled, y_shuffled = X[indices], y[indices]\n",
        "\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                batch_X = X_shuffled[i:i + batch_size]\n",
        "                batch_y = y_shuffled[i:i + batch_size]\n",
        "\n",
        "                self.forward(batch_X)\n",
        "                self.backward(batch_X, batch_y, learning_rate)\n",
        "\n",
        "            # Apply learning rate decay\n",
        "            learning_rate = learning_rate / (1 + decay_rate * epoch)\n",
        "\n",
        "            # Compute loss and log\n",
        "            predictions = self.forward(X)\n",
        "            loss = np.mean((y - predictions) ** 2)\n",
        "            losses.append(loss)\n",
        "\n",
        "            if verbose and epoch % 100 == 0:\n",
        "                logging.info(f\"Epoch {epoch}/{epochs} - Loss: {loss:.6f}\")\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
        "        \"\"\"Evaluate the network.\"\"\"\n",
        "        predictions = self.forward(X)\n",
        "        loss = np.mean((y - predictions) ** 2)\n",
        "        accuracy = np.mean((predictions > 0.5) == y)\n",
        "        return loss, accuracy\n",
        "\n",
        "    def save_model(self, filepath: str) -> None:\n",
        "        \"\"\"Save model weights and biases.\"\"\"\n",
        "        model_data = {\n",
        "            \"weights\": [w.tolist() for w in self.weights],\n",
        "            \"biases\": [b.tolist() for b in self.biases]\n",
        "        }\n",
        "        with open(filepath, \"w\") as f:\n",
        "            json.dump(model_data, f)\n",
        "\n",
        "    def load_model(self, filepath: str) -> None:\n",
        "        \"\"\"Load model weights and biases.\"\"\"\n",
        "        with open(filepath, \"r\") as f:\n",
        "            model_data = json.load(f)\n",
        "        self.weights = [np.array(w) for w in model_data[\"weights\"]]\n",
        "        self.biases = [np.array(b) for b in model_data[\"biases\"]]\n",
        "\n",
        "    def plot_training_history(self, losses: List[float]) -> None:\n",
        "        \"\"\"Plot training loss.\"\"\"\n",
        "        plt.plot(losses, label=\"Loss\")\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Example Usage\n",
        "def main():\n",
        "    \"\"\"Example usage with the XOR problem.\"\"\"\n",
        "    # XOR problem dataset\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([0, 1, 1, 0])\n",
        "\n",
        "    try:\n",
        "        # Create network\n",
        "        nn = NeuralNetwork(input_size=2, hidden_layers_sizes=[4], output_size=1, activations=[\"relu\", \"sigmoid\"])\n",
        "\n",
        "        # Train network\n",
        "        losses = nn.train(X, y, epochs=5000, learning_rate=0.1, decay_rate=0.01, batch_size=4)\n",
        "\n",
        "        # Evaluate and print results\n",
        "        loss, accuracy = nn.evaluate(X, y)\n",
        "        print(\"\\nFinal Results:\")\n",
        "        print(f\"Loss: {loss:.6f}\")\n",
        "        print(f\"Accuracy: {accuracy:.2%}\\n\")\n",
        "\n",
        "        # Show predictions\n",
        "        print(\"Predictions:\")\n",
        "        predictions = nn.forward(X)\n",
        "        for i in range(len(X)):\n",
        "            predicted_value = predictions[i][0]\n",
        "            expected_value = y[i]\n",
        "            print(f\"Input: {X[i]} -> Predicted: {predicted_value:.4f} (Expected: {expected_value})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HwrLJdG5qFfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}