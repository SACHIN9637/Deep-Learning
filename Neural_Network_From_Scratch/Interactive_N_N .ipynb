{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Interactive_N_N"
      ],
      "metadata": {
        "id": "mMUASzmDnBUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a modular feedforward neural network with the following features:\n",
        "\n",
        "1)Layer Class:\n",
        "\n",
        "Represents individual layers with customizable activation functions (ReLU, Sigmoid, Tanh).\n",
        "Handles forward propagation and activation operations.\n",
        "\n",
        "2)NeuralNetwork Class:\n",
        "\n",
        "Manages the network architecture with multiple layers.\n",
        "\n",
        "Supports:\n",
        "\n",
        "Forward Propagation: Computes predictions.\n",
        "Backward Propagation: Updates weights using gradient descent.\n",
        "Training: Mini-batch training with loss and accuracy tracking.\n",
        "Prediction: Outputs class probabilities or predictions.\n",
        "Tracks training history and plots loss and accuracy.\n",
        "\n",
        "3)Save/Load Functionality:\n",
        "\n",
        "Save model architecture, weights, and biases to a JSON file.\n",
        "Load and reconstruct models from saved files.\n",
        "\n",
        "4)Visualization:\n",
        "\n",
        "Plots loss and accuracy trends during training."
      ],
      "metadata": {
        "id": "pzz_7YaonFfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "87HN_JDRmPzr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Optional\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):\n",
        "        \"\"\"Initialize a neural network layer with weights and biases\"\"\"\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.biases = np.zeros((1, output_size))\n",
        "        self.activation = activation\n",
        "\n",
        "        # Cache for backpropagation\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        self.activation_output = None\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward pass through the layer\"\"\"\n",
        "        self.input = X\n",
        "        self.output = np.dot(X, self.weights) + self.biases\n",
        "        self.activation_output = self._activate(self.output)\n",
        "        return self.activation_output\n",
        "\n",
        "    def _activate(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply activation function\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif self.activation == 'tanh':\n",
        "            return np.tanh(x)\n",
        "        raise ValueError(f\"Unsupported activation function: {self.activation}\")\n",
        "\n",
        "    def _activate_derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute derivative of activation function\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            s = 1 / (1 + np.exp(-x))\n",
        "            return s * (1 - s)\n",
        "        elif self.activation == 'tanh':\n",
        "            return 1 - np.tanh(x)**2\n",
        "        raise ValueError(f\"Unsupported activation function: {self.activation}\")\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes: List[int], activations: List[str]):\n",
        "        \"\"\"Initialize neural network with specified layer sizes and activations\"\"\"\n",
        "        if len(layer_sizes) < 2:\n",
        "            raise ValueError(\"Need at least input and output layers\")\n",
        "        if len(layer_sizes) - 1 != len(activations):\n",
        "            raise ValueError(\"Number of activations must match number of layers - 1\")\n",
        "\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(Layer(layer_sizes[i], layer_sizes[i + 1], activations[i]))\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward pass through entire network\"\"\"\n",
        "        current_output = X\n",
        "        for layer in self.layers:\n",
        "            current_output = layer.forward(current_output)\n",
        "        return current_output\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01):\n",
        "        \"\"\"Backward pass for parameter updates\"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Compute initial gradient\n",
        "        output = self.layers[-1].activation_output\n",
        "        delta = output - y\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            layer = self.layers[i]\n",
        "\n",
        "            # Compute gradients\n",
        "            if i != len(self.layers) - 1:  # Hidden layers\n",
        "                delta = np.dot(delta, self.layers[i + 1].weights.T) * layer._activate_derivative(layer.output)\n",
        "\n",
        "            # Update parameters\n",
        "            layer.weights -= learning_rate * np.dot(layer.input.T, delta) / m\n",
        "            layer.biases -= learning_rate * np.sum(delta, axis=0, keepdims=True) / m\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, learning_rate: float = 0.01,\n",
        "             batch_size: Optional[int] = None, verbose: bool = True) -> None:\n",
        "        \"\"\"Train the neural network\"\"\"\n",
        "        m = X.shape[0]\n",
        "        batch_size = batch_size if batch_size else m\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Mini-batch training\n",
        "            for i in range(0, m, batch_size):\n",
        "                batch_X = X[i:i + batch_size]\n",
        "                batch_y = y[i:i + batch_size]\n",
        "\n",
        "                # Forward and backward passes\n",
        "                self.forward(batch_X)\n",
        "                self.backward(batch_X, batch_y, learning_rate)\n",
        "\n",
        "            # Track metrics\n",
        "            predictions = self.predict(X)\n",
        "            loss = self.compute_loss(predictions, y)\n",
        "            accuracy = self.compute_accuracy(predictions, y)\n",
        "\n",
        "            self.loss_history.append(loss)\n",
        "            self.accuracy_history.append(accuracy)\n",
        "\n",
        "            if verbose and epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "    def compute_loss(self, predictions: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"Compute MSE loss\"\"\"\n",
        "        return np.mean(np.square(predictions - y))\n",
        "\n",
        "    def compute_accuracy(self, predictions: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"Compute classification accuracy\"\"\"\n",
        "        return np.mean(np.argmax(predictions, axis=1) == np.argmax(y, axis=1))\n",
        "\n",
        "    def plot_training_history(self) -> None:\n",
        "        \"\"\"Plot training metrics history\"\"\"\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.loss_history)\n",
        "        plt.title('Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.accuracy_history)\n",
        "        plt.title('Training Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_model(self, filepath: str) -> None:\n",
        "        \"\"\"Save model parameters to file\"\"\"\n",
        "        model_params = {\n",
        "            'architecture': [layer.weights.shape[0] for layer in self.layers] +\n",
        "                          [self.layers[-1].weights.shape[1]],\n",
        "            'activations': [layer.activation for layer in self.layers],\n",
        "            'weights': [layer.weights.tolist() for layer in self.layers],\n",
        "            'biases': [layer.biases.tolist() for layer in self.layers]\n",
        "        }\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(model_params, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath: str) -> 'NeuralNetwork':\n",
        "        \"\"\"Load model from file\"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            model_params = json.load(f)\n",
        "\n",
        "        network = cls(model_params['architecture'], model_params['activations'])\n",
        "        for i, layer in enumerate(network.layers):\n",
        "            layer.weights = np.array(model_params['weights'][i])\n",
        "            layer.biases = np.array(model_params['biases'][i])\n",
        "\n",
        "        return network"
      ]
    }
  ]
}